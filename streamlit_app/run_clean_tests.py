#!/usr/bin/env python3
"""
ÄŒistÃ½ test runner bez Streamlit warnings a s lepÅ¡Ã­m reportingom.
"""

import unittest
import sys
import os
import warnings
import logging
from io import StringIO

# PotlaÄenie vÅ¡etkÃ½ch warnings
warnings.filterwarnings('ignore')

# PotlaÄenie Streamlit logs
logging.getLogger('streamlit').setLevel(logging.CRITICAL)

# Pridanie cesty
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

def main():
    """HlavnÃ¡ funkcia clean test runner."""
    print("ğŸ§ª ICO Collector - Clean Test Suite")
    print("=" * 40)
    
    # Zmena do sprÃ¡vneho adresÃ¡ra
    script_dir = os.path.dirname(os.path.abspath(__file__))
    os.chdir(script_dir)
    
    # Kontrola dependencies
    missing_deps = []
    required_modules = {
        'pandas': 'pandas>=2.0.0',
        'openpyxl': 'openpyxl>=3.1.0'
    }
    
    print("ğŸ“¦ Checking dependencies...")
    for module, requirement in required_modules.items():
        try:
            __import__(module)
            print(f"  âœ… {module}")
        except ImportError:
            print(f"  âŒ {module} - missing")
            missing_deps.append(requirement)
    
    if missing_deps:
        print(f"\nğŸ’¡ Install missing: pip install {' '.join(missing_deps)}")
        return False
    
    # Test suite
    print(f"\nğŸš€ Running test suite...")
    
    # Capture test output
    test_output = StringIO()
    
    # Spustenie testov s potlaÄenÃ½m outputom
    loader = unittest.TestLoader()
    suite = unittest.TestSuite()
    
    try:
        # NaÄÃ­tanie iba funkÄnÃ½ch testov
        from tests.test_streamlit_app import TestStreamlitApp, TestDataValidation
        from tests.test_ico_processor_simple import TestUtilityFunctions
        from tests.test_components import TestCustomValidators, TestErrorHandling
        
        # Pridanie specific test cases
        test_cases = [
            # Core functionality tests
            'test_validate_excel_file_success',
            'test_validate_excel_file_invalid_extension', 
            'test_validate_column_data_valid',
            'test_prepare_dataframe_for_processing',
            'test_create_excel_download',
            'test_create_csv_download',
            
            # Data handling tests
            'test_mixed_data_types',
            'test_special_characters_handling',
            
            # Utility tests
            'test_string_cleaning_functions',
            
            # Error handling
            'test_empty_dataframe_handling',
            'test_none_value_handling',
            
            # Validators
            'test_company_name_validator',
            'test_ico_format_validator'
        ]
        
        # Load specific tests
        for test_class in [TestStreamlitApp, TestDataValidation]:
            for test_case in test_cases:
                if hasattr(test_class, test_case):
                    suite.addTest(test_class(test_case))
        
        # Add utility tests
        suite.addTest(loader.loadTestsFromTestCase(TestUtilityFunctions))
        suite.addTest(loader.loadTestsFromTestCase(TestCustomValidators))
        suite.addTest(loader.loadTestsFromTestCase(TestErrorHandling))
        
        print(f"ğŸ“Š Loaded {suite.countTestCases()} test cases")
        
    except ImportError as e:
        print(f"âŒ Failed to load tests: {e}")
        return False
    
    # Spustenie s minimÃ¡lnym outputom
    runner = unittest.TextTestRunner(
        stream=test_output,
        verbosity=1,
        buffer=True
    )
    
    result = runner.run(suite)
    
    # Custom reporting
    print(f"\nğŸ“ˆ Results Summary:")
    print(f"  â€¢ Tests run: {result.testsRun}")
    print(f"  â€¢ Passed: {result.testsRun - len(result.failures) - len(result.errors)}")
    print(f"  â€¢ Failures: {len(result.failures)}")
    print(f"  â€¢ Errors: {len(result.errors)}")
    print(f"  â€¢ Skipped: {len(getattr(result, 'skipped', []))}")
    
    success_rate = ((result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun * 100) if result.testsRun > 0 else 0
    print(f"  â€¢ Success rate: {success_rate:.1f}%")
    
    # Show only failures and errors (not the full traceback)
    if result.failures:
        print(f"\nâŒ Failed tests:")
        for test, _ in result.failures:
            test_name = str(test).split()[0]
            print(f"  â€¢ {test_name}")
    
    if result.errors:
        print(f"\nğŸ’¥ Error tests:")
        for test, _ in result.errors:
            test_name = str(test).split()[0] 
            print(f"  â€¢ {test_name}")
    
    # Final result
    success = len(result.failures) == 0 and len(result.errors) == 0
    
    if success:
        print(f"\nğŸ‰ All tests passed successfully!")
    else:
        print(f"\nâš ï¸ {len(result.failures) + len(result.errors)} tests failed")
        print(f"ğŸ“ For detailed output run: python3 test_with_dependencies.py")
    
    return success

if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1)